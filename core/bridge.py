import json
from openai import OpenAI # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
from typing import List, Dict
from db.neo4j_handler import Neo4jHandler
from .innovation_engine import find_innovative_path
from .verify_causal import verify_causal_path # ØªÙ… Ø§ÙØªØ±Ø§Ø¶ ÙˆØ¬ÙˆØ¯ TRUST_THRESHOLD Ù‡Ù†Ø§
from .weights import update_system_confidence, update_causal_weight

# ÙŠØ¬Ø¨ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù…ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨
# client = OpenAI(api_key=...) 

# 1. ØªØµÙ…ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ (JSON Schema)
CAUSAL_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "cause": {"type": "string", "description": "Ø§Ø³Ù… Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø°ÙŠ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙØ¹Ù„ Ø£Ùˆ Ø§Ù„Ø­Ø§Ù„Ø©."},
            "effect": {"type": "string", "description": "Ø§Ø³Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø£Ùˆ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø©."},
            "claim_type": {"type": "string", "description": "Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ù…Ø²Ø¹ÙˆÙ…Ø© (Ù…Ø«Ù„: CAUSESØŒ PREVENTSØŒ ENABLES)."}
        },
        "required": ["cause", "effect"]
    }
}

# ------------------------------------------------------------------
# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§)
# ------------------------------------------------------------------

def extract_causal_claims_from_llm(llm_output_text: str, client: OpenAI) -> List[Dict]:
    """ÙŠØ³ØªØ®Ø¯Ù… LLM Ù„ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© Ø§Ù„Ù…Ù†Ø¸Ù…Ø©."""
    # ... (Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ. Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¹ÙŠØ¯ 'claims' Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ ÙˆÙ„ÙŠØ³ claims.get("claims", []))
    system_prompt = ("Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ù†Ø·Ù‚ÙŠ Ù…ØªØ®ØµØµ. ...")
    user_content = f"Ø§Ù„Ù†Øµ Ù„ØªØ­Ù„ÙŠÙ„Ù‡: '{llm_output_text}'"

    try:
        response = client.chat.completions.create(
            model="gpt-4-turbo", 
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
            response_format={"type": "json_object"}, 
        )
        raw_json_output = response.choices[0].message.content
        claims_data = json.loads(raw_json_output)
        
        # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø®Ø±Ø¬ Ù‡Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ùˆ Ø¯Ø§Ø®Ù„ Ù…ÙØªØ§Ø­ 'claims'
        if isinstance(claims_data, list):
            return claims_data
        elif isinstance(claims_data, dict) and 'claims' in claims_data:
            return claims_data['claims']
        
        return []

    except Exception as e:
        print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ù…Ù† LLM: {e}")
        return []
    

def generate_exploratory_question(llm_client: OpenAI, cause: str, effect: str, threshold: float) -> str:
    """ÙŠÙˆÙ„Ø¯ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ÙˆØ¬Ù‡Ø§Ù‹ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ø¨Ø¨ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø³Ø¨Ø¨ ÙˆØ§Ù„Ù†ØªÙŠØ¬Ø©."""
    # ... (Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ)
    system_prompt = ("Ø£Ù†Øª Ù…Ø­Ù‚Ù‚ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø¨Ø¨ÙŠ. ...")
    user_content = (f"Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø¥Ø«Ø¨Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ Ø£Ù† '{cause}' ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ '{effect}' "
                    f"Ù„Ø£Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹. Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø£Ù† Ø£Ø³Ø£Ù„ Ø¹Ù†Ù‡ØŸ")

    try:
        response = llm_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}]
        )
        question = response.choices[0].message.content
        print(f"**[GAP ALERT]** ØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙØ¬ÙˆØ© Ø³Ø¨Ø¨ÙŠØ© Ø¨ÙŠÙ† {cause} Ùˆ {effect}. Thresh={threshold}")
        return f"Ù†Ø­ØªØ§Ø¬ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙØ¬ÙˆØ© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©: {question}"

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ Ø§Ù„Ø¢Ù† Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ÙÙŠ LLM: {e}"


# ------------------------------------------------------------------
# 2. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¬Ø³Ø± (process_and_learn)
# ------------------------------------------------------------------

def process_and_learn(llm_text: str, handler: Neo4jHandler, llm_client: OpenAI, feedback_delta: float = 0.0):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ù„Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§ØªØŒ ØªØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ØŒ ÙˆØªØ¯ÙŠØ± Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ.
    """
    
    # 1. Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù†Øµ
    causal_claims = extract_causal_claims_from_llm(llm_text, llm_client) 
    
    verified_path = None
    best_claim = None
    
    if causal_claims:
        # Ù†Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ ÙØ±Ø¶ÙŠØ© ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§
        best_claim = causal_claims[0] 
        verified_path = verify_causal_path(handler, best_claim['cause'], best_claim['effect'])
    
    # ------------------------------------------------------------------
    # 2. Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ
    # ------------------------------------------------------------------
    
    if verified_path:
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹
        
        # â­ 2.1. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©)
        if feedback_delta != 0.0:
            update_causal_weight(handler, verified_path['path_details'], feedback_delta)
            
        # â­ 2.2. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ (Ø§Ù„Ù†Ø¬Ø§Ø­ ÙŠØ¹Ø²Ø² Ø§Ù„Ø«Ù‚Ø©)
        new_confidence = update_system_confidence(handler, success_delta=0.1) # ØªØ¹Ø²ÙŠØ² Ø¨Ø³ÙŠØ·
        
        return {
            "status": "Success - Logically Verified and Learned",
            "message": "ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø¨Ø¨ÙŠ. ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø£Ù…Ø§Ù†.",
            "system_confidence": new_confidence
        }
    
    else:
        # Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„: Ø§ÙƒØªØ´Ø§Ù ÙØ¬ÙˆØ© Ø³Ø¨Ø¨ÙŠØ© (Hallucination Prevention)
        
        # â­ 2.3. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ (Ø§Ù„ÙØ´Ù„ ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø«Ù‚Ø©)
        new_confidence = update_system_confidence(handler, success_delta=-0.2) # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚
        
        if best_claim:
            # 2.4. ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø·
            gap_question = generate_exploratory_question(
                llm_client, 
                best_claim['cause'], 
                best_claim['effect'], 
                verify_causal_path.TRUST_THRESHOLD
            )
            return {
                "status": "Failure - Causal Gap Found (Active Learning)",
                "action_required": "Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
                "question": gap_question,
                "system_confidence": new_confidence
            }
        else:
            return {
                "status": "Failure - No Claims Found", 
                "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙØ±Ø¶ÙŠØ§Øª Ø³Ø¨Ø¨ÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§.",
                "system_confidence": new_confidence
            }


def attempt_innovative_solution(handler: Neo4jHandler, llm_client: OpenAI, original_cause: str, desired_effect: str):
    # ... (Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ)
    # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ÙŠÙˆØ¯
    constraints_to_ignore = ["High_Cost", "Slow_Protocol_K", "Mandatory_Check_J"]
    
    print(f"\n[ğŸš€ INNOVATION MODE] ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙÙƒÙŠØ± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ ÙŠØªØ¬Ø§Ù‡Ù„: {constraints_to_ignore}")
    
    # 2. ØªØ·Ø¨ÙŠÙ‚ Ù…Ø´ØºÙ„ imagine(I)
    innovative_path = find_innovative_path(
        handler,
        start_entity=original_cause,
        target_goal=desired_effect,
        constraints_to_ignore=constraints_to_ignore
    )
    
    if innovative_path:
        return {
            "status": "Innovative Solution Found",
            "path": innovative_path['path_details'],
            "risk_assessment": "Ù…Ø·Ù„ÙˆØ¨ ØªØ­Ù„ÙŠÙ„ Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ø¬Ù„."
        }
    else:
        return {"status": "Innovation Failed", "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ù„ Ø§Ø¨ØªÙƒØ§Ø±ÙŠ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚."}