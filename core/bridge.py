import json
from openai import OpenAI # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
from typing import List, Dict
from db.neo4j_handler import Neo4jHandler
from .innovation_engine import find_innovative_path
from .verify_causal import verify_causal_path, TRUST_THRESHOLD 
from .weights import update_system_confidence, update_causal_weight

# ÙŠØ¬Ø¨ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù…ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨
# client = OpenAI(api_key=...) 

# 1. ØªØµÙ…ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ (JSON Schema)
CAUSAL_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "cause": {"type": "string", "description": "Ø§Ø³Ù… Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø°ÙŠ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙØ¹Ù„ Ø£Ùˆ Ø§Ù„Ø­Ø§Ù„Ø©."},
            "effect": {"type": "string", "description": "Ø§Ø³Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø£Ùˆ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø©."},
            "claim_type": {"type": "string", "description": "Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ù…Ø²Ø¹ÙˆÙ…Ø© (Ù…Ø«Ù„: CAUSESØŒ PREVENTSØŒ ENABLES)."}
        },
        "required": ["cause", "effect"]
    }
}

# ------------------------------------------------------------------
# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§)
# ------------------------------------------------------------------

def extract_causal_claims_from_llm(llm_output_text: str, client: OpenAI) -> List[Dict]:
    """ÙŠØ³ØªØ®Ø¯Ù… LLM Ù„ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© Ø§Ù„Ù…Ù†Ø¸Ù…Ø©."""
    
    system_prompt = (
        "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ù†Ø·Ù‚ÙŠ Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© (cause -> effect) "
        "Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙ‚Ø¯Ù…. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø®Ø±Ø¬ **Ø¨ØµÙŠØºØ© JSON** ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…Ø®Ø·Ø· CAUSAL_SCHEMA." # â­ ØªÙ… Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© JSON
    )
    user_content = f"Ø§Ù„Ù†Øµ Ù„ØªØ­Ù„ÙŠÙ„Ù‡: '{llm_output_text}'"

    try:
        response = client.chat.completions.create(
            model="gpt-4-turbo", 
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
            response_format={"type": "json_object"}, 
        )
        raw_json_output = response.choices[0].message.content
        claims_data = json.loads(raw_json_output)
        
        # â­ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø­Ø§Ø³Ù…: ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ù…ØºÙ„ÙØ© Ø¨Ù…ÙØªØ§Ø­
        if isinstance(claims_data, list):
            return claims_data
        elif isinstance(claims_data, dict) and 'causal_claims' in claims_data: # â­ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ 'claims' Ø¨Ù€ 'causal_claims'
            return claims_data['causal_claims']
        
        return []

    except Exception as e:
        print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ù…Ù† LLM: {e}")
        return []
    

def generate_exploratory_question(llm_client: OpenAI, cause: str, effect: str, threshold: float) -> str:
    """ÙŠÙˆÙ„Ø¯ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ÙˆØ¬Ù‡Ø§Ù‹ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ø¨Ø¨ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø³Ø¨Ø¨ ÙˆØ§Ù„Ù†ØªÙŠØ¬Ø©."""
    # ... (Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ)
    system_prompt = ("Ø£Ù†Øª Ù…Ø­Ù‚Ù‚ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø¨Ø¨ÙŠ. ...")
    user_content = (f"Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø¥Ø«Ø¨Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ Ø£Ù† '{cause}' ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ '{effect}' "
                    f"Ù„Ø£Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹. Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø£Ù† Ø£Ø³Ø£Ù„ Ø¹Ù†Ù‡ØŸ")

    try:
        response = llm_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}]
        )
        question = response.choices[0].message.content
        print(f"**[GAP ALERT]** ØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙØ¬ÙˆØ© Ø³Ø¨Ø¨ÙŠØ© Ø¨ÙŠÙ† {cause} Ùˆ {effect}. Thresh={threshold}")
        return f"Ù†Ø­ØªØ§Ø¬ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙØ¬ÙˆØ© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©: {question}"

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ Ø§Ù„Ø¢Ù† Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ÙÙŠ LLM: {e}"


# ------------------------------------------------------------------
# 2. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¬Ø³Ø± (process_and_learn)
# ------------------------------------------------------------------

def process_and_learn(llm_text: str, handler: Neo4jHandler, llm_client: OpenAI, feedback_delta: float = 0.0):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ù„Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§ØªØŒ ØªØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ØŒ ÙˆØªØ¯ÙŠØ± Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ.
    """
    
    # 1. Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù†Øµ
    causal_claims = extract_causal_claims_from_llm(llm_text, llm_client) 
    
    verified_path = None
    best_claim = None
    
    if causal_claims:
        # Ù†Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ ÙØ±Ø¶ÙŠØ© ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§
        best_claim = causal_claims[0] 
        verified_path = verify_causal_path(handler, best_claim['cause'], best_claim['effect'])
    
    # ------------------------------------------------------------------
    # 2. Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ
    # ------------------------------------------------------------------
    
    if verified_path:
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹
        
        # â­ 2.1. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©)
        if feedback_delta != 0.0:
            update_causal_weight(handler, verified_path['path_details'], feedback_delta)
            
        # â­ 2.2. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ (Ø§Ù„Ù†Ø¬Ø§Ø­ ÙŠØ¹Ø²Ø² Ø§Ù„Ø«Ù‚Ø©)
        new_confidence = update_system_confidence(handler, success_delta=0.1) # ØªØ¹Ø²ÙŠØ² Ø¨Ø³ÙŠØ·
        
        return {
            "status": "Success - Logically Verified and Learned",
            "message": "ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø¨Ø¨ÙŠ. ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø£Ù…Ø§Ù†.",
            "system_confidence": new_confidence
        }
    
    else:
        # Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„: Ø§ÙƒØªØ´Ø§Ù ÙØ¬ÙˆØ© Ø³Ø¨Ø¨ÙŠØ© (Hallucination Prevention)
        
        # â­ 2.3. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ (Ø§Ù„ÙØ´Ù„ ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø«Ù‚Ø©)
        new_confidence = update_system_confidence(handler, success_delta=-0.2) # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚
        
        if best_claim:
            # 2.4. ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø·
            gap_question = generate_exploratory_question(
                llm_client, 
                best_claim['cause'], 
                best_claim['effect'], 
                TRUST_THRESHOLD
            )
            return {
                "status": "Failure - Causal Gap Found (Active Learning)",
                "action_required": "Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
                "question": gap_question,
                "system_confidence": new_confidence
            }
        else:
            return {
                "status": "Failure - No Claims Found", 
                "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙØ±Ø¶ÙŠØ§Øª Ø³Ø¨Ø¨ÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§.",
                "system_confidence": new_confidence
            }


# ÙÙŠ core/bridge.py (Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©)

def assess_innovative_risk(llm_client: OpenAI, path_details: List[Dict]) -> Dict:
    """
    ØªÙ‚ÙŠÙŠÙ… Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±ÙŠ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM.

    Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:
        llm_client: ÙƒØ§Ø¦Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ LLM.
        path_details: ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±ÙŠ (Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©/Ø§Ù„Ù…ÙØªØ¬Ø§Ù‡Ù„Ø©).
        
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
        Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± (Risk Score, Side Effects).
    """
    
    path_summary = "\n".join([f"- {e['start']} -> {e['end']} (Weight: {e.get('weight', 'NEW')})" for e in path_details])
    
    system_prompt = (
        "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±. ØªÙ… Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ø³Ø§Ø± Ø³Ø¨Ø¨ÙŠ Ø¬Ø¯ÙŠØ¯ (Ø§Ø¨ØªÙƒØ§Ø±ÙŠ) ÙŠØªØ¬Ø§ÙˆØ² "
        "Ø¨Ø¹Ø¶ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù‚Ø¯Ù… ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨ ÙÙŠÙ‡Ø§ "
        "ÙˆÙ…Ø®Ø§Ø·Ø± Ø§Ù„ØªÙ†ÙÙŠØ° Ø¹Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ Ù…Ù† 0 (Ù…Ø®Ø§Ø·Ø±Ø© Ù…Ø¹Ø¯ÙˆÙ…Ø©) Ø¥Ù„Ù‰ 1.0 (Ø®Ø·Ø± Ø´Ø¯ÙŠØ¯). "
        "ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø®Ø±Ø¬ Ø¨ØµÙŠØºØ© **JSON** Ø¨Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: {'risk_score': float, 'side_effects': str}."
    )
    
    user_content = (
        f"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±ÙŠ Ø§Ù„Ù…Ù‚ØªØ±Ø­ (Ø§Ù„Ø±ÙˆØ§Ø¨Ø·): \n{path_summary}\n"
        f"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (Ù…Ø«Ù„: Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„ØªÙƒÙ„ÙØ©ØŒ ØªØ¯Ù‡ÙˆØ± Ø§Ù„Ø£Ø¯Ø§Ø¡)ØŸ"
    )

    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØµÙŠØ§ØºØ© Ø§Ù„Ø®Ø±Ø¬ ÙƒÙ€ JSON
        response = llm_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
            response_format={"type": "json_object"},
        )
        
        raw_json = response.choices[0].message.content
        risk_data = json.loads(raw_json)
        
        # ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙˆÙ‚Ø¹ Ø§Ù„Ù€ Schema Ù…ÙØªØ§Ø­ÙŠÙ† Ø±Ø¦ÙŠØ³ÙŠÙŠÙ†: risk_score Ùˆ side_effects
        return {
            "risk_score": risk_data.get('risk_score', 0.5), # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            "side_effects": risk_data.get('side_effects', "Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø¢Ø«Ø§Ø± Ø¬Ø§Ù†Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.")
        }
    
    except Exception as e:
        print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¹Ø¨Ø± LLM: {e}")
        return {"risk_score": 1.0, "side_effects": "ÙØ´Ù„ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±ØŒ ÙŠØ¬Ø¨ Ø±ÙØ¶ Ø§Ù„Ø­Ù„."}
    

# ÙÙŠ core/bridge.py (ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© attempt_innovative_solution)

def attempt_innovative_solution(handler: Neo4jHandler, llm_client: OpenAI, original_cause: str, desired_effect: str):
    
    # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ÙŠÙˆØ¯ (I) Ø§Ù„ØªÙŠ Ù…Ù†Ø¹Øª Ø§Ù„Ø­Ù„ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
    constraints_to_ignore = ["High_Cost", "Slow_Protocol_K", "Mandatory_Check_J"]
    
    print(f"\n[ğŸš€ INNOVATION MODE] ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙÙƒÙŠØ± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ ÙŠØªØ¬Ø§Ù‡Ù„: {constraints_to_ignore}")
    
    # 2. ØªØ·Ø¨ÙŠÙ‚ Ù…Ø´ØºÙ„ imagine(I)
    innovative_path = find_innovative_path(
        handler,
        start_entity=original_cause,
        target_goal=desired_effect,
        constraints_to_ignore=constraints_to_ignore
    )
    
    if innovative_path:
        # â­ 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± (Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
        risk_assessment = assess_innovative_risk(llm_client, innovative_path['path_details'])
        
        risk_score = risk_assessment['risk_score']
        
        if risk_score > 0.7:
            # Ø±ÙØ¶ Ø§Ù„Ø­Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
            return {
                "status": "Innovative Solution REJECTED",
                "message": f"ØªÙ… Ø±ÙØ¶ Ø§Ù„Ø­Ù„ Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±ÙŠ Ø¨Ø³Ø¨Ø¨ Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ù…Ø®Ø§Ø·Ø± (Risk Score: {risk_score}).",
                "risk_details": risk_assessment['side_effects']
            }
        
        # 4. Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø­Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ù…Ù‚Ø¨ÙˆÙ„Ø©
        return {
            "status": "Innovative Solution Found",
            "path": innovative_path['path_details'],
            "risk_assessment": risk_assessment # ØªØ¶Ù…ÙŠÙ† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙƒØ§Ù…Ù„
        }
    else:
        return {"status": "Innovation Failed", "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ù„ Ø§Ø¨ØªÙƒØ§Ø±ÙŠ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚."}
    
