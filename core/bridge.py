import json
from openai import OpenAI # ูุซุงู ุนูู ุงุณุชุฎุฏุงู LLM
from typing import List, Dict
from db.neo4j_handler import Neo4jHandler
from .innovation_engine import find_innovative_path
from .verify_causal import verify_causal_path

# ูุฌุจ ุชููุฆุฉ ุงูุนููู ูู ููุงู ููุงุณุจ (ูุซู ููู ุชููุฆุฉ ุนุงู)
# client = OpenAI(api_key=...) 

# 1. ุชุตููู ูููู ุงูุจูุงูุงุช ุงููุชููุน (JSON Schema)
CAUSAL_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "cause": {"type": "string", "description": "ุงุณู ุงูุดูุก ุงูุฐู ูุณุจุจ ุงููุนู ุฃู ุงูุญุงูุฉ."},
            "effect": {"type": "string", "description": "ุงุณู ุงููุชูุฌุฉ ุฃู ุงูุญุงูุฉ ุงููุงุชุฌุฉ."},
            "claim_type": {"type": "string", "description": "ููุน ุงูุนูุงูุฉ ุงููุฒุนููุฉ (ูุซู: CAUSESุ PREVENTSุ ENABLES)."}
        },
        "required": ["cause", "effect"]
    }
}

# ุชุญุฏูุซ ุฏุงูุฉ process_llm_output ูู core/bridge.py (ูููู ุฌุฏูุฏ)

from .verify_causal import verify_causal_path
from .weights import update_causal_weight

# ุชุญุฏูุซ ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ ูู core/bridge.py

from .verify_causal import verify_causal_path
# ... ุงุณุชูุฑุงุฏ ุฏุงูุฉ extract_causal_claims_from_llm ...

def process_llm_output(llm_text: str, handler: Neo4jHandler, llm_client: OpenAI):
    """
    ุงูููุทู ุงูุฑุฆูุณู ููุฌุณุฑ: ูุณุชุฎูุต ุงููุฑุถูุงุช ููุชุญูู ูููุง ุณุจุจููุง.
    """
    
    # 1. ุงุณุชุฎูุงุต ุงููุฑุถูุงุช ูู ุงููุต (ุงูุทุจูุฉ ุงูุนุตุจูุฉ)
    causal_claims = extract_causal_claims_from_llm(llm_text, llm_client) 
    
    verified_claims = []
    
    for claim in causal_claims:
        # 2. ุงูุชุญูู ูู ูู ูุฑุถูุฉ ุถุฏ ุงูุฐุงูุฑุฉ Z (ุงูุทุจูุฉ ุงูุทูุจูููุฌูุฉ)
        verified_path = verify_causal_path(handler, claim['cause'], claim['effect'])
        
        if verified_path:
            # 3. ูุจูู ุงููุฑุถูุฉ ุงูููุซููุฉ
            claim['is_verified'] = True
            claim['weight'] = verified_path['path_weight']
            verified_claims.append(claim)
        else:
            # 4. ุฑูุถ ุงููุฑุถูุฉ ุงูุถุนููุฉ ุฃู ุงููุงุฐุจุฉ
            claim['is_verified'] = False
            verified_claims.append(claim)
    
    # 5. ุจูุงุก ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ ุจูุงุกู ุนูู ุงููุชุงุฆุฌ ุงููุชุญูู ูููุง
    if verified_claims:
        # ููุง ูููู ุงุณุชุฎุฏุงู ุงูู LLM ูุฑุฉ ุฃุฎุฑู ูุตูุงุบุฉ ุฅุฌุงุจุฉ ููุงุฆูุฉ ุฏูููุฉ ููุชุญูู ูููุง
        # ุฃู ูููู ุงูุงูุชูุงุก ุจุงูุฑูุงุจุท ุงููุคูุฏุฉ ูู ูุฐู ุงููุฑุญูุฉ
        return {
            "status": "Success - Logically Verified",
            "verified_claims": verified_claims,
            "raw_llm_output": llm_text
        }
    else:
        # ูุชู ุชูุนูู ุขููุฉ ุงูุชุนูู ุงููุดุท (ุทุฑุญ ุงูุฃุณุฆูุฉ) ุฅุฐุง ูู ูุชู ุงูุชุญูู ูู ุฃู ุดูุก
        return {
            "status": "Failure - Causal Gaps Found",
            "suggestion": "ูุฌุจ ุทุฑุญ ุณุคุงู ุงุณุชูุดุงูู ูุณุฏ ุงููุฌูุฉ ูู ุงูุฐุงูุฑุฉ ุงูุณุจุจูุฉ Z."
        }
    
# ุชุญุฏูุซ ุฏุงูุฉ process_llm_output ูู core/bridge.py (ูููู ุฌุฏูุฏ)
# ุชุญุฏูุซ ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ process_and_learn ูู core/bridge.py

def process_and_learn(llm_text: str, handler: Neo4jHandler, llm_client: OpenAI, feedback_delta: float = 0.0):
    
    # ... (1. ุงุณุชุฎูุงุต ุงููุฑุถูุงุช) ...
    causal_claims = extract_causal_claims_from_llm(llm_text, llm_client) 
    
    # ... (2. ุงูุชุญูู ูู ุงููุฑุถูุงุช ูุงุฎุชูุงุฑ ุงููุณุงุฑ ุงูุฃูุซู) ...
    # ููุชุฑุถ ููุง ุฃููุง ูุจุญุซ ููุท ุนู ุฃูู ูุฑุถูุฉ (ูุชุจุณูุท ุงููุซุงู)
    
    verified_path = None
    if causal_claims:
        claim = causal_claims[0]
        verified_path = verify_causal_path(handler, claim['cause'], claim['effect'])
    
    # ------------------------------------------------------------------
    # 3. ุงุชุฎุงุฐ ุงููุฑุงุฑ ุจุนุฏ ุงูุชุญูู
    # ------------------------------------------------------------------
    if verified_path:
        # ุฅุฐุง ุชู ุงูุชุญูู ุจูุฌุงุญ
        # ... (ูุฑุญูุฉ ุงูุชุนูู ูุงูู update_causal_weight ุฅุฐุง ูุงูุช ููุงู ุชุบุฐูุฉ ุฑุงุฌุนุฉ) ...
        return {
            "status": "Success - Logically Verified",
            "message": "ุชู ุชุฃููุฏ ุงูููุทู ุงูุณุจุจู. ูููู ุชูููุฐ ุงููุฑุงุฑ ุจุฃูุงู."
        }
    else:
        # ุฅุฐุง ูุดู ุงูุชุญูู (ุงูุชุดุงู ุงููุฌูุฉ ุงูุณุจุจูุฉ)
        
        # 4. ุชูููุฏ ุณุคุงู ููุชุนูู ุงููุดุท
        if causal_claims:
            gap_question = generate_exploratory_question(
                llm_client, 
                claim['cause'], 
                claim['effect'], 
                verify_causal_path.TRUST_THRESHOLD # ุงุณุชุฎุฏุงู ุงูุนุชุจุฉ ูู ุฏุงูุฉ ุงูุชุญูู
            )
            return {
                "status": "Failure - Causal Gap Found (Active Learning)",
                "action_required": "ุทูุจ ูุนูููุงุช ูู ุงููุณุชุฎุฏู",
                "question": gap_question
            }
        else:
             return {"status": "Failure - No Claims Found", "message": "ูู ูุชู ุงูุนุซูุฑ ุนูู ูุฑุถูุงุช ุณุจุจูุฉ ููุชุญูู ูููุง."}


def extract_causal_claims_from_llm(llm_output_text: str, client: OpenAI) -> List[Dict]:
    """
    ูุณุชุฎุฏู LLM ูุชุญููู ูุต ุงูุฅุฌุงุจุฉ ูุงุณุชุฎูุงุต ุงููุฑุถูุงุช ุงูุณุจุจูุฉ ุงูููุธูุฉ (Causes, Effects).
    
    ุงููุฏุฎูุงุช:
        llm_output_text: ุงูุฅุฌุงุจุฉ ุงูุฃูููุฉ ุงูููุชุฑุญุฉ ูู LLM.
        client: ูุงุฆู ุงูุนููู ุงูุฎุงุต ุจู LLM.
        
    ุงููุฎุฑุฌุงุช:
        ูุงุฆูุฉ ุจูุฑุถูุงุช {cause, effect, claim_type}.
    """
    
    # 2. ุจูุงุก ุฃูุฑ ุงูุชุญุฑูุถ (Prompt)
    system_prompt = (
        "ุฃูุช ูุญูู ููุทูู ูุชุฎุตุต. ูููุชู ูู ุชุญููู ุงููุต ุงูุชุงูู ูุงุณุชุฎูุงุต ุฌููุน ุงูุนูุงูุงุช ุงูุณุจุจูุฉ "
        "(ุงูุณุจุจ ูุงููุชูุฌุฉ) ุงููุงุฑุฏุฉ ููู. ูุฌุจ ุฃู ูููู ุงููุงุชุฌ ุจุตูุบุฉ JSON ูุชุทุงุจู ุชูุงููุง ูุน ุงููููู ุงูููุฏู."
    )
    
    # ุฏูุฌ ุงููุต ุงูุฐู ูุฌุจ ุชุญูููู
    user_content = f"ุงููุต ูุชุญูููู: '{llm_output_text}'"

    try:
        # 3. ุฅุฑุณุงู ุงูุทูุจ ูููููุฐุฌ (ุจุงุณุชุฎุฏุงู ุฃุฏูุงุช ุงุณุชุฏุนุงุก ุงููุธุงุฆู ูุชูุธูู JSON)
        response = client.chat.completions.create(
            model="gpt-4-turbo",  # ููุถู ูููุฐุฌ ููู ูุชูุธูู JSON ุจุฏูุฉ
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            # ุงุณุชุฎุฏุงู ุชูููุฉ JSON mode ุฃู tools/functions ุฅุฐุง ูุงูุช ูุชุงุญุฉ ูุถูุงู ุชูุณูู ุงูุฎุฑุฌ
            response_format={"type": "json_object"}, 
            # *ููุงุญุธุฉ: ูู ูุฐุง ุงููุซุงูุ ููุชุฑุถ ุฃู ุงููููุฐุฌ ุณูุฎุฑุฌ JSON ุจุดูู ูุจุงุดุฑ*
        )
        
        # 4. ุชุญููู ุงูุฎุฑุฌ
        raw_json_output = response.choices[0].message.content
        claims = json.loads(raw_json_output)
        
        # ูุฏ ูุญุชุงุฌ ุฅูู ุจุนุถ ุงูุชูุธูู ุฃู ุงูุชุญูู ูู ุตุญุฉ Schema
        return claims.get("claims", []) # ููุชุฑุถ ุฃู ุงูุฎุฑุฌ ูุฏ ูุญุชูู ุนูู ููุชุงุญ ุฑุฆูุณู 'claims'

    except Exception as e:
        print(f"ุญุฏุซ ุฎุทุฃ ูู ุงุณุชุฎูุงุต ุงููุฑุถูุงุช ูู LLM: {e}")
        return []
    

def generate_exploratory_question(
    llm_client: OpenAI, 
    cause: str, 
    effect: str, 
    threshold: float
) -> str:
    """
    ูููุฏ ุณุคุงูุงู ููุฌูุงู ูููุณุชุฎุฏู ูุทูุจ ูุนูููุงุช ุณุจุจูุฉ ูุญุฏุฏุฉ ุจูู ุงูุณุจุจ ูุงููุชูุฌุฉ.

    ุงููุฏุฎูุงุช:
        llm_client: ูุงุฆู ุงูุนููู ุงูุฎุงุต ุจู LLM.
        cause: ุงูุณุจุจ ุงูุฐู ูู ูุชู ุงูุชุญูู ููู.
        effect: ุงููุชูุฌุฉ ุงูุชู ูู ูุชู ุงูุชุญูู ูููุง.
        threshold: ุนุชุจุฉ ุงูุซูุฉ (tau) ุงูุชู ูู ูุชู ุงููุตูู ุฅูููุง.

    ุงููุฎุฑุฌุงุช:
        ุณุคุงู ุงุณุชูุดุงูู ูุญุฏุฏ.
    """
    
    system_prompt = (
        "ุฃูุช ูุญูู ูุชุฎุตุต ูู ุงูููุทู ุงูุณุจุจู. ูููุชู ูู ุตูุงุบุฉ ุณุคุงู ุฏููู ุฌุฏุงู ูููุณุชุฎุฏู "
        "ูุทูุจ ูุนูููุฉ ูุญุฏุฏุฉ ุชุณุฏ ูุฌูุฉ ุจูู [ุงูุณุจุจ] ู [ุงููุชูุฌุฉ]. ูุฌุจ ุฃู ูุฑูุฒ ุงูุณุคุงู ุนูู "
        "ุงูุจุฑูุชูููู ุฃู ุงูุฎุทูุฉ ุงูููููุฏุฉ ุงูุชู ุชุฑุจุทููุง."
    )
    
    user_content = (
        f"ุงููุดููุฉ: ูุง ุฃุณุชุทูุน ุฅุซุจุงุช ููุทููุงู ุฃู '{cause}' ูุคุฏู ุฅูู '{effect}' "
        f"ูุฃู ุงูุฑูุงุจุท ุงูุญุงููุฉ ุถุนููุฉ ุฌุฏุงู. ูุง ูู ุงูุฅุฌุฑุงุก ุงูููููุฏ ุงูุฐู ูุฌุจ ุฃู ุฃุณุฃู ุนููุ"
    )

    try:
        response = llm_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ]
        )
        
        # 1. ุชุญููู ุงูุฅุฌุงุจุฉ ูุชุถููููุง
        question = response.choices[0].message.content
        
        # 2. ุชุญุฏูุฏ ุงููุฌูุฉ ูู ุงูุฐุงูุฑุฉ (ููุชูุซูู ุงูุฏุงุฎูู)
        print(f"**[GAP ALERT]** ุชู ุงูุชุดุงู ูุฌูุฉ ุณุจุจูุฉ ุจูู {cause} ู {effect}. Thresh={threshold}")
        
        return f"ูุญุชุงุฌ ูููุณุงุนุฏุฉ ูู ุฅุบูุงู ุงููุฌูุฉ ุงููุนุฑููุฉ: {question}"

    except Exception as e:
        return f"ุนุฐุฑุงูุ ูุง ูููููู ุตูุงุบุฉ ุณุคุงู ุงุณุชูุดุงูู ุงูุขู ุจุณุจุจ ุฎุทุฃ ูู LLM: {e}"
    

# ุฌุฒุก ูู ุฏุงูุฉ ุงุชุฎุงุฐ ุงููุฑุงุฑ ูู core/bridge.py

def attempt_innovative_solution(handler, llm_client, original_cause, desired_effect):
    
    # 1. ุชุญุฏูุฏ ุงููููุฏ (I) ุงูุชู ููุนุช ุงูุญู ุงูุชูููุฏู (ูุซูุงู: ูููุฏ ุงูุฃุฏุงุกุ ูููุฏ ุงูุชูููุฉุ ุฅูุฎ)
    # *ููุง ูุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู LLM ูุชุญุฏูุฏ ุงููููุฏ ุจูุงุกู ุนูู ุณูุงู ุงููุดููุฉ*
    constraints_to_ignore = ["High_Cost", "Slow_Protocol_K", "Mandatory_Check_J"]
    
    print(f"\n[๐ INNOVATION MODE] ุชุญููู ุงูุชูููุฑ ููุจุญุซ ุนู ุญู ูุชุฌุงูู: {constraints_to_ignore}")
    
    # 2. ุชุทุจูู ูุดุบู imagine(I)
    innovative_path = find_innovative_path(
        handler,
        start_entity=original_cause,
        target_goal=desired_effect,
        constraints_to_ignore=constraints_to_ignore
    )
    
    if innovative_path:
        # 3. ุชูููู ุงูุงุจุชูุงุฑ
        # ููุง ูุฌุจ ุงุณุชุฎุฏุงู LLM ูุชูููู ุงููุฎุงุทุฑ (ุงูุขุซุงุฑ ุงูุฌุงูุจูุฉ) ูุจู ุงูุชูุตูุฉ
        
        return {
            "status": "Innovative Solution Found",
            "path": innovative_path['path_details'],
            "risk_assessment": "ูุทููุจ ุชุญููู ูุฎุงุทุฑ ุนุงุฌู."
        }
    else:
        return {"status": "Innovation Failed", "message": "ูู ูุชู ุงูุนุซูุฑ ุนูู ุญู ุงุจุชูุงุฑู ูุงุจู ููุชุทุจูู."}
    